{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p12 = pd.read_excel(\"p12_2008_2016.xlsx\")\n",
    "\n",
    "df_el = pd.read_excel(\"dfcleaned.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\patricio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\patricio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  \\\n",
      "0  Como consecuencia del impacto, el hombre que i...   \n",
      "1   Por Paula Kearney, El Programa de Promoción F...   \n",
      "2  Otra de las cuestiones sobre las que trabajan ...   \n",
      "3   Por María del Carmen Arias * , La noción de t...   \n",
      "4  Edición:   anterior   \\n siguiente   , © 2000-...   \n",
      "\n",
      "                                     cleaned_content  \n",
      "0  consecuencia impacto hombre iba acompañante co...  \n",
      "1  paula kearney programa promoción familiar area...  \n",
      "2  cuestiones trabajan operadores calle consumo d...  \n",
      "3  maría carmen arias noción toxicomanía sufrido ...  \n",
      "4  edición anterior siguiente wwwpaginacomar repú...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# Drop rows with missing content\n",
    "df_p12 = df_p12.dropna(subset=['content'])\n",
    "\n",
    "# Convert the content to lowercase\n",
    "df_p12['cleaned_content'] = df_p12['content'].str.lower()\n",
    "\n",
    "# Remove special characters, numbers, and punctuation\n",
    "df_p12['cleaned_content'] = df_p12['cleaned_content'].apply(lambda x: re.sub(r'[^a-zA-Z\\sáéíóúüñÁÉÍÓÚÜÑ]', '', x))\n",
    "\n",
    "# Tokenize the text and remove stop words\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "df_p12['cleaned_content'] = df_p12['cleaned_content'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))\n",
    "\n",
    "# Remove extra whitespaces\n",
    "df_p12['cleaned_content'] = df_p12['cleaned_content'].apply(lambda x: ' '.join(x.split()))\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(df_p12[['content', 'cleaned_content']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_el[\"media\"] = \"litoral\"\n",
    "df_p12[\"media\"] = \"p12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date = \"2008-1-1\"\n",
    "end_date = \"2015-12-31\"\n",
    "mask = (df_p12['date'] > start_date) & (df_p12['date'] <= end_date)\n",
    "\n",
    "df_p12 = df_p12.loc[mask]\n",
    "len(df_p12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p12.drop(columns=[\"position\", \"day\", \"month\", \"year\", \"description\"], inplace=True, errors=\"ignore\")\n",
    "df_el.drop(columns=[\"query\", \"subtitle\"], inplace=True, errors=\"ignore\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'title', 'url', 'content', 'cleaned_content', 'media'], dtype='object')\n",
      "Index(['date', 'title', 'url', 'content', 'cleaned_content', 'media'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_p12.columns)\n",
    "print(df_el.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "media\n",
       "litoral    527\n",
       "p12        360\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df_el, df_p12], axis=0, ignore_index=True)\n",
    "df.value_counts(\"media\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['cleaned_content'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRUEBA 2008-2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patricio\\AppData\\Local\\Temp\\ipykernel_2428\\2719764351.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['cleaned_content'] = df1['cleaned_content'].apply(lemmatize)\n",
      "C:\\Users\\patricio\\AppData\\Local\\Temp\\ipykernel_2428\\2719764351.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['topic_probabilities'] = lda.transform(X).tolist()\n",
      "c:\\Users\\patricio\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\patricio\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:819: FutureWarning: 'square_distances' has been introduced in 0.24 to help phase out legacy squaring behavior. The 'legacy' setting will be removed in 1.1 (renaming of 0.26), and the default setting will be changed to True. In 1.3, 'square_distances' will be removed altogether, and distances will be squared by default. Set 'square_distances'=True to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "end_date = \"2011-12-31\"\n",
    "mask = (df['date'] <= end_date)\n",
    "\n",
    "df1 = df.loc[mask]\n",
    "\n",
    "\n",
    "\n",
    "# Descargar el modelo en español de spaCy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "# Definir las stopwords en español\n",
    "stop_words_spanish = spacy.lang.es.stop_words.STOP_WORDS\n",
    "stop_words_spanish.update([\"año\", \"wwwpaginacomar\", \"software\", \"desarrollado\", \"gnulinux\"])\n",
    "# Realizar lematización en español\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc if token.text.lower() not in stop_words_spanish])\n",
    "\n",
    "# Lematizar el contenido limpio\n",
    "df1['cleaned_content'] = df1['cleaned_content'].apply(lemmatize)\n",
    "\n",
    "# Vectorizar el contenido limpio\n",
    "vectorizer = CountVectorizer(max_features=1000, lowercase=True, stop_words=stop_words_spanish)\n",
    "X = vectorizer.fit_transform(df1['cleaned_content'])\n",
    "\n",
    "# Aplicar Latent Dirichlet Allocation (LDA)\n",
    "num_topics = 8  # Puedes ajustar el número de temas\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Asignar las probabilidades del tema a cada documento\n",
    "df1['topic_probabilities'] = lda.transform(X).tolist()\n",
    "\n",
    "import pyLDAvis.lda_model\n",
    "pyLDAvis.enable_notebook()\n",
    "prueba = pyLDAvis.lda_model.prepare(lda, X, vectorizer, mds='tsne', n_jobs=1)\n",
    "pyLDAvis.save_html(prueba, \"2008-2011_tsne.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\patricio\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\patricio\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:819: FutureWarning: 'square_distances' has been introduced in 0.24 to help phase out legacy squaring behavior. The 'legacy' setting will be removed in 1.1 (renaming of 0.26), and the default setting will be changed to True. In 1.3, 'square_distances' will be removed altogether, and distances will be squared by default. Set 'square_distances'=True to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.lda_model\n",
    "pyLDAvis.enable_notebook()\n",
    "prueba = pyLDAvis.lda_model.prepare(lda, X, vectorizer, mds='tsne', n_jobs=1)\n",
    "pyLDAvis.save_html(prueba, \"2008-2011_tsne.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patricio\\AppData\\Local\\Temp\\ipykernel_2428\\3968579397.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['cleaned_content'] = df2['cleaned_content'].apply(lemmatize)\n",
      "C:\\Users\\patricio\\AppData\\Local\\Temp\\ipykernel_2428\\3968579397.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['topic_probabilities'] = lda.transform(X).tolist()\n",
      "c:\\Users\\patricio\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\patricio\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:819: FutureWarning: 'square_distances' has been introduced in 0.24 to help phase out legacy squaring behavior. The 'legacy' setting will be removed in 1.1 (renaming of 0.26), and the default setting will be changed to True. In 1.3, 'square_distances' will be removed altogether, and distances will be squared by default. Set 'square_distances'=True to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "start_date = \"2011-12-31\"\n",
    "mask = (df['date'] > start_date)\n",
    "\n",
    "df2 = df.loc[mask]\n",
    "\n",
    "\n",
    "\n",
    "# Descargar el modelo en español de spaCy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "# Definir las stopwords en español\n",
    "stop_words_spanish = spacy.lang.es.stop_words.STOP_WORDS\n",
    "# Realizar lematización en español\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc if token.text.lower() not in stop_words_spanish])\n",
    "\n",
    "# Lematizar el contenido limpio\n",
    "df2['cleaned_content'] = df2['cleaned_content'].apply(lemmatize)\n",
    "\n",
    "# Vectorizar el contenido limpio\n",
    "vectorizer = CountVectorizer(max_features=1000, lowercase=True, stop_words=stop_words_spanish)\n",
    "X = vectorizer.fit_transform(df2['cleaned_content'])\n",
    "\n",
    "# Aplicar Latent Dirichlet Allocation (LDA)\n",
    "num_topics = 8  # Puedes ajustar el número de temas\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Asignar las probabilidades del tema a cada documento\n",
    "df2['topic_probabilities'] = lda.transform(X).tolist()\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "prueba = pyLDAvis.lda_model.prepare(lda, X, vectorizer, mds='tsne', n_jobs=1)\n",
    "pyLDAvis.save_html(prueba, \"2012-2016_tsne.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\patricio\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\patricio\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:819: FutureWarning: 'square_distances' has been introduced in 0.24 to help phase out legacy squaring behavior. The 'legacy' setting will be removed in 1.1 (renaming of 0.26), and the default setting will be changed to True. In 1.3, 'square_distances' will be removed altogether, and distances will be squared by default. Set 'square_distances'=True to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "prueba = pyLDAvis.lda_model.prepare(lda, X, vectorizer, mds='tsne', n_jobs=1)\n",
    "pyLDAvis.save_html(prueba, \"2012-2016_tsne.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelado por medio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Litoral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patricio\\AppData\\Local\\Temp\\ipykernel_2428\\1241119130.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_el['cleaned_content'] = df_el['cleaned_content'].apply(lemmatize)\n",
      "C:\\Users\\patricio\\AppData\\Local\\Temp\\ipykernel_2428\\1241119130.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_el['topic_probabilities'] = lda.transform(X).tolist()\n",
      "c:\\Users\\patricio\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\patricio\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:819: FutureWarning: 'square_distances' has been introduced in 0.24 to help phase out legacy squaring behavior. The 'legacy' setting will be removed in 1.1 (renaming of 0.26), and the default setting will be changed to True. In 1.3, 'square_distances' will be removed altogether, and distances will be squared by default. Set 'square_distances'=True to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df_el = df[df[\"media\"] == \"litoral\"]\n",
    "\n",
    "# Definir las stopwords en español\n",
    "stop_words_spanish = spacy.lang.es.stop_words.STOP_WORDS\n",
    "# Realizar lematización en español\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc if token.text.lower() not in stop_words_spanish])\n",
    "\n",
    "# Lematizar el contenido limpio\n",
    "df_el['cleaned_content'] = df_el['cleaned_content'].apply(lemmatize)\n",
    "\n",
    "# Vectorizar el contenido limpio\n",
    "vectorizer = CountVectorizer(max_features=1000, lowercase=True, stop_words=stop_words_spanish)\n",
    "X = vectorizer.fit_transform(df_el['cleaned_content'])\n",
    "\n",
    "# Aplicar Latent Dirichlet Allocation (LDA)\n",
    "num_topics = 8  # Puedes ajustar el número de temas\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Asignar las probabilidades del tema a cada documento\n",
    "df_el['topic_probabilities'] = lda.transform(X).tolist()\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "prueba = pyLDAvis.lda_model.prepare(lda, X, vectorizer, mds='tsne', n_jobs=1)\n",
    "pyLDAvis.save_html(prueba, \"ellitoral.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Página 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patricio\\AppData\\Local\\Temp\\ipykernel_2428\\2722564612.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_p12['cleaned_content'] = df_p12['cleaned_content'].apply(lemmatize)\n",
      "C:\\Users\\patricio\\AppData\\Local\\Temp\\ipykernel_2428\\2722564612.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_p12['topic_probabilities'] = lda.transform(X).tolist()\n",
      "c:\\Users\\patricio\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\patricio\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:819: FutureWarning: 'square_distances' has been introduced in 0.24 to help phase out legacy squaring behavior. The 'legacy' setting will be removed in 1.1 (renaming of 0.26), and the default setting will be changed to True. In 1.3, 'square_distances' will be removed altogether, and distances will be squared by default. Set 'square_distances'=True to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df_p12 = df[df[\"media\"] == \"p12\"]\n",
    "\n",
    "# Definir las stopwords en español\n",
    "stop_words_spanish = spacy.lang.es.stop_words.STOP_WORDS\n",
    "# Realizar lematización en español\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc if token.text.lower() not in stop_words_spanish])\n",
    "\n",
    "# Lematizar el contenido limpio\n",
    "df_p12['cleaned_content'] = df_p12['cleaned_content'].apply(lemmatize)\n",
    "\n",
    "# Vectorizar el contenido limpio\n",
    "vectorizer = CountVectorizer(max_features=1000, lowercase=True, stop_words=stop_words_spanish)\n",
    "X = vectorizer.fit_transform(df_p12['cleaned_content'])\n",
    "\n",
    "# Aplicar Latent Dirichlet Allocation (LDA)\n",
    "num_topics = 8  # Puedes ajustar el número de temas\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Asignar las probabilidades del tema a cada documento\n",
    "df_p12['topic_probabilities'] = lda.transform(X).tolist()\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "prueba = pyLDAvis.lda_model.prepare(lda, X, vectorizer, mds='tsne', n_jobs=1)\n",
    "pyLDAvis.save_html(prueba, \"p12.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Por diario y por período"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Litoral 2008-2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\patricio\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\patricio\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:819: FutureWarning: 'square_distances' has been introduced in 0.24 to help phase out legacy squaring behavior. The 'legacy' setting will be removed in 1.1 (renaming of 0.26), and the default setting will be changed to True. In 1.3, 'square_distances' will be removed altogether, and distances will be squared by default. Set 'square_distances'=True to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df_el = df[df[\"media\"] == \"litoral\"]\n",
    "\n",
    "end_date = \"2011-12-31\"\n",
    "mask = (df_el['date'] <= end_date)\n",
    "\n",
    "df_el = df_el.loc[mask]\n",
    "\n",
    "\n",
    "# Definir las stopwords en español\n",
    "stop_words_spanish = spacy.lang.es.stop_words.STOP_WORDS\n",
    "# Realizar lematización en español\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc if token.text.lower() not in stop_words_spanish])\n",
    "\n",
    "# Lematizar el contenido limpio\n",
    "df_el['cleaned_content'] = df_el['cleaned_content'].apply(lemmatize)\n",
    "\n",
    "# Vectorizar el contenido limpio\n",
    "vectorizer = CountVectorizer(max_features=1000, lowercase=True, stop_words=stop_words_spanish)\n",
    "X = vectorizer.fit_transform(df_el['cleaned_content'])\n",
    "\n",
    "# Aplicar Latent Dirichlet Allocation (LDA)\n",
    "num_topics = 8  # Puedes ajustar el número de temas\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Asignar las probabilidades del tema a cada documento\n",
    "df_el['topic_probabilities'] = lda.transform(X).tolist()\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "prueba = pyLDAvis.lda_model.prepare(lda, X, vectorizer, mds='tsne', n_jobs=1)\n",
    "pyLDAvis.save_html(prueba, \"ellitoral_2008-2011.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Litoral 2011-2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\patricio\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\patricio\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:819: FutureWarning: 'square_distances' has been introduced in 0.24 to help phase out legacy squaring behavior. The 'legacy' setting will be removed in 1.1 (renaming of 0.26), and the default setting will be changed to True. In 1.3, 'square_distances' will be removed altogether, and distances will be squared by default. Set 'square_distances'=True to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df_el = df[df[\"media\"] == \"litoral\"]\n",
    "\n",
    "start_date = \"2011-12-31\"\n",
    "mask = (df_el['date'] > start_date)\n",
    "\n",
    "df_el = df_el.loc[mask]\n",
    "\n",
    "\n",
    "# Definir las stopwords en español\n",
    "stop_words_spanish = spacy.lang.es.stop_words.STOP_WORDS\n",
    "# Realizar lematización en español\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc if token.text.lower() not in stop_words_spanish])\n",
    "\n",
    "# Lematizar el contenido limpio\n",
    "df_el['cleaned_content'] = df_el['cleaned_content'].apply(lemmatize)\n",
    "\n",
    "# Vectorizar el contenido limpio\n",
    "vectorizer = CountVectorizer(max_features=1000, lowercase=True, stop_words=stop_words_spanish)\n",
    "X = vectorizer.fit_transform(df_el['cleaned_content'])\n",
    "\n",
    "# Aplicar Latent Dirichlet Allocation (LDA)\n",
    "num_topics = 8  # Puedes ajustar el número de temas\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Asignar las probabilidades del tema a cada documento\n",
    "df_el['topic_probabilities'] = lda.transform(X).tolist()\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "prueba = pyLDAvis.lda_model.prepare(lda, X, vectorizer, mds='tsne', n_jobs=1)\n",
    "pyLDAvis.save_html(prueba, \"ellitoral_2011-2016.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Página12 2008-2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'spacy.lang' has no attribute 'es'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2428\\3265644397.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Definir las stopwords en español\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mstop_words_spanish\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSTOP_WORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;31m# Realizar lematización en español\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'spacy.lang' has no attribute 'es'"
     ]
    }
   ],
   "source": [
    "df_p12 = df[df[\"media\"] == \"p12\"]\n",
    "\n",
    "end_date = \"2011-12-31\"\n",
    "mask = (df_p12['date'] <= end_date)\n",
    "\n",
    "df_p12 = df_p12.loc[mask]\n",
    "\n",
    "# Descargar el modelo en español de spaCy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Definir las stopwords en español\n",
    "stop_words_spanish = spacy.lang.es.stop_words.STOP_WORDS\n",
    "# Realizar lematización en español\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc if token.text.lower() not in stop_words_spanish])\n",
    "\n",
    "# Lematizar el contenido limpio\n",
    "df_p12['cleaned_content'] = df_p12['cleaned_content'].apply(lemmatize)\n",
    "\n",
    "# Vectorizar el contenido limpio\n",
    "vectorizer = CountVectorizer(max_features=1000, lowercase=True, stop_words=stop_words_spanish)\n",
    "X = vectorizer.fit_transform(df_p12['cleaned_content'])\n",
    "\n",
    "# Aplicar Latent Dirichlet Allocation (LDA)\n",
    "num_topics = 4  # Puedes ajustar el número de temas\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Asignar las probabilidades del tema a cada documento\n",
    "df_p12['topic_probabilities'] = lda.transform(X).tolist()\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "prueba = pyLDAvis.lda_model.prepare(lda, X, vectorizer, mds='tsne', n_jobs=1)\n",
    "pyLDAvis.save_html(prueba, \"p12_2008-2011.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Página12 2011-2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pyLDAvis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2428\\1095746261.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mdf_p12\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'topic_probabilities'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[0mprueba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tsne'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_html\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprueba\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"p12_2011-2016.html\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pyLDAvis' is not defined"
     ]
    }
   ],
   "source": [
    "df_p12 = df[df[\"media\"] == \"p12\"]\n",
    "\n",
    "start_date = \"2011-12-31\"\n",
    "mask = (df_p12['date'] > start_date)\n",
    "\n",
    "df_p12 = df_p12.loc[mask]\n",
    "\n",
    "# Descargar el modelo en español de spaCy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Definir las stopwords en español\n",
    "stop_words_spanish = spacy.lang.es.stop_words.STOP_WORDS\n",
    "# Realizar lematización en español\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc if token.text.lower() not in stop_words_spanish])\n",
    "\n",
    "# Lematizar el contenido limpio\n",
    "df_p12['cleaned_content'] = df_p12['cleaned_content'].apply(lemmatize)\n",
    "\n",
    "# Vectorizar el contenido limpio\n",
    "vectorizer = CountVectorizer(max_features=1000, lowercase=True, stop_words=stop_words_spanish)\n",
    "X = vectorizer.fit_transform(df_p12['cleaned_content'])\n",
    "\n",
    "# Aplicar Latent Dirichlet Allocation (LDA)\n",
    "num_topics = 8  # Puedes ajustar el número de temas\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Asignar las probabilidades del tema a cada documento\n",
    "df_p12['topic_probabilities'] = lda.transform(X).tolist()\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "prueba = pyLDAvis.lda_model.prepare(lda, X, vectorizer, mds='tsne', n_jobs=1)\n",
    "pyLDAvis.save_html(prueba, \"p12_2011-2016.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_p12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_p12 = df[df[\"media\"] == \"p12\"]\n",
    "\n",
    "end_date = \"2011-12-31\"\n",
    "mask = (df_p12['date'] <= end_date)\n",
    "\n",
    "df_p12 = df_p12.loc[mask]\n",
    "len(df_p12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
