{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import spacy\n",
    "import pyLDAvis.lda_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with El Litoral and Rosario12 news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we load the datasets and clean them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset with news between 2008 and 2016 from Rosario12\n",
    "df_p12 = pd.read_excel(\"p12_2008_2016.xlsx\")\n",
    "\n",
    "#dataset with news between 2008 and 2020 from El Litoral\n",
    "df_el = pd.read_excel(\"el_2008_2020.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\patricio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\patricio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>cleaned_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Como consecuencia del impacto, el hombre que i...</td>\n",
       "      <td>consecuencia impacto hombre iba acompañante co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Por Paula Kearney, El Programa de Promoción F...</td>\n",
       "      <td>paula kearney programa promoción familiar area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Otra de las cuestiones sobre las que trabajan ...</td>\n",
       "      <td>cuestiones trabajan operadores calle consumo d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Por María del Carmen Arias * , La noción de t...</td>\n",
       "      <td>maría carmen arias noción toxicomanía sufrido ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edición:   anterior   \\n siguiente   , © 2000-...</td>\n",
       "      <td>edición anterior siguiente wwwpaginacomar repú...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Como consecuencia del impacto, el hombre que i...   \n",
       "1   Por Paula Kearney, El Programa de Promoción F...   \n",
       "2  Otra de las cuestiones sobre las que trabajan ...   \n",
       "3   Por María del Carmen Arias * , La noción de t...   \n",
       "4  Edición:   anterior   \\n siguiente   , © 2000-...   \n",
       "\n",
       "                                     cleaned_content  \n",
       "0  consecuencia impacto hombre iba acompañante co...  \n",
       "1  paula kearney programa promoción familiar area...  \n",
       "2  cuestiones trabajan operadores calle consumo d...  \n",
       "3  maría carmen arias noción toxicomanía sufrido ...  \n",
       "4  edición anterior siguiente wwwpaginacomar repú...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# drop rows with missing content\n",
    "df_p12 = df_p12.dropna(subset=['content'])\n",
    "\n",
    "# convert the content to lowercase\n",
    "df_p12['cleaned_content'] = df_p12['content'].str.lower()\n",
    "\n",
    "# remove special characters, numbers, and punctuation\n",
    "df_p12['cleaned_content'] = df_p12['cleaned_content'].apply(lambda x: re.sub(r'[^a-zA-Z\\sáéíóúüñÁÉÍÓÚÜÑ]', '', x))\n",
    "\n",
    "# tokenize the text and remove stop words\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "df_p12['cleaned_content'] = df_p12['cleaned_content'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))\n",
    "\n",
    "# remove extra whitespaces\n",
    "df_p12['cleaned_content'] = df_p12['cleaned_content'].apply(lambda x: ' '.join(x.split()))\n",
    "\n",
    "# display the cleaned dataframe\n",
    "df_p12[['content', 'cleaned_content']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column to each dataframe with the media the data belongs to\n",
    "\n",
    "df_el[\"media\"] = \"litoral\"\n",
    "df_p12[\"media\"] = \"p12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#limit entrys to dates that are part of our analysis and check how many news there are\n",
    "start_date = \"2008-1-1\"\n",
    "end_date = \"2015-12-31\"\n",
    "mask = (df_p12['date'] > start_date) & (df_p12['date'] <= end_date)\n",
    "\n",
    "df_p12 = df_p12.loc[mask]\n",
    "len(df_p12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'title', 'url', 'content', 'cleaned_content', 'media'], dtype='object')\n",
      "Index(['date', 'title', 'url', 'content', 'cleaned_content', 'media'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# drop irrelevant columns and check if both datasets have the same columns\n",
    "df_p12.drop(columns=[\"position\", \"day\", \"month\", \"year\", \"description\"], inplace=True, errors=\"ignore\")\n",
    "df_el.drop(columns=[\"query\", \"subtitle\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "print(df_p12.columns)\n",
    "print(df_el.columns)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "media\n",
       "litoral    510\n",
       "p12        360\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concat dataframes, drop rows with missing content and check how many articles we have for each media\n",
    "df = pd.concat([df_el, df_p12], axis=0, ignore_index=True)\n",
    "\n",
    "df.dropna(subset=['cleaned_content'], inplace=True)\n",
    "\n",
    "df.value_counts(\"media\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling with LDA (2008-2011)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we lemmatize the content and vectorize it. The result is a matrix (X)  where each row of this matrix corresponds to a document (text sample), and each column corresponds to a word in the vocabulary. The values in the matrix represent the count of each word in the corresponding document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = \"2011-12-31\"\n",
    "mask = (df['date'] <= end_date)\n",
    "\n",
    "df1 = df.loc[mask]\n",
    "\n",
    "\n",
    "\n",
    "# loading spacy model for spanish language\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# seting stopwords\n",
    "stop_words_spanish = spacy.lang.es.stop_words.STOP_WORDS\n",
    "stop_words_spanish.update([\"año\", \"wwwpaginacomar\", \"software\", \"desarrollado\", \"gnulinux\"])\n",
    "\n",
    "# function to lemmatize\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc if token.text.lower() not in stop_words_spanish])\n",
    "\n",
    "df1['cleaned_content'] = df1['cleaned_content'].apply(lemmatize)\n",
    "\n",
    "# vectorize the content\n",
    "vectorizer = CountVectorizer(max_features=1000, lowercase=True, stop_words=stop_words_spanish)\n",
    "X = vectorizer.fit_transform(df1['cleaned_content'])\n",
    "\n",
    "# apply Latent Dirichlet Allocation \n",
    "num_topics = 8  # select number of topics\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# assign topic probabilities to each article\n",
    "df1['topic_probabilities'] = lda.transform(X).tolist()\n",
    "\n",
    "#using pyLDAvis library to visualize in html file the relevance of topics and words associated with it\n",
    "\n",
    "import pyLDAvis.lda_model\n",
    "pyLDAvis.enable_notebook()\n",
    "prueba = pyLDAvis.lda_model.prepare(lda, X, vectorizer, mds='tsne', n_jobs=1)\n",
    "pyLDAvis.save_html(prueba, \"2008-2011_tsne.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result we have for each article the probability of it belonging to the 8 topics we modeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>topic_probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Ahora se incauta más droga porque hay más tar...</td>\n",
       "      <td>[0.6126970146102731, 0.0004909926346014362, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"El enemigo es el narcotráfico, no el Frente P...</td>\n",
       "      <td>[0.0018708806101210432, 0.33596309539764335, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\"La cosa no es con ustedes\": el mensaje que pu...</td>\n",
       "      <td>[0.0027835253492482083, 0.002783806147025475, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>\"Por la Asignación Universal por Hijo aumentar...</td>\n",
       "      <td>[0.0018409389082352148, 0.9871139737318255, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Acribillan a familia en venganza por la muerte...</td>\n",
       "      <td>[0.411420612680842, 0.3371789524310031, 0.0007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>La persecución del consumo de drogas según pas...</td>\n",
       "      <td>[0.00016981241123813426, 0.0001698211561314628...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>La provincia apunta al narcotráfico</td>\n",
       "      <td>[0.6377249216477957, 0.0012648295414492655, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>Cocaína en bolsitas y fierros</td>\n",
       "      <td>[0.00076834845069267, 0.0007683318082665085, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>Asalto de cumbia en el penal de Rosario</td>\n",
       "      <td>[0.2566642343511511, 0.14100446204355174, 0.15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.7401051790947729, 0.0010986612983098258, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>139 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "4    \"Ahora se incauta más droga porque hay más tar...   \n",
       "6    \"El enemigo es el narcotráfico, no el Frente P...   \n",
       "14   \"La cosa no es con ustedes\": el mensaje que pu...   \n",
       "21   \"Por la Asignación Universal por Hijo aumentar...   \n",
       "31   Acribillan a familia en venganza por la muerte...   \n",
       "..                                                 ...   \n",
       "582  La persecución del consumo de drogas según pas...   \n",
       "583                La provincia apunta al narcotráfico   \n",
       "584                     Cocaína en bolsitas y fierros    \n",
       "585            Asalto de cumbia en el penal de Rosario   \n",
       "586                                                NaN   \n",
       "\n",
       "                                   topic_probabilities  \n",
       "4    [0.6126970146102731, 0.0004909926346014362, 0....  \n",
       "6    [0.0018708806101210432, 0.33596309539764335, 0...  \n",
       "14   [0.0027835253492482083, 0.002783806147025475, ...  \n",
       "21   [0.0018409389082352148, 0.9871139737318255, 0....  \n",
       "31   [0.411420612680842, 0.3371789524310031, 0.0007...  \n",
       "..                                                 ...  \n",
       "582  [0.00016981241123813426, 0.0001698211561314628...  \n",
       "583  [0.6377249216477957, 0.0012648295414492655, 0....  \n",
       "584  [0.00076834845069267, 0.0007683318082665085, 0...  \n",
       "585  [0.2566642343511511, 0.14100446204355174, 0.15...  \n",
       "586  [0.7401051790947729, 0.0010986612983098258, 0....  \n",
       "\n",
       "[139 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[['title', 'topic_probabilities']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2011-2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2011-12-31\"\n",
    "mask = (df['date'] > start_date)\n",
    "\n",
    "df2 = df.loc[mask]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "stop_words_spanish = spacy.lang.es.stop_words.STOP_WORDS\n",
    "# Realizar lematización en español\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc if token.text.lower() not in stop_words_spanish])\n",
    "\n",
    "\n",
    "df2['cleaned_content'] = df2['cleaned_content'].apply(lemmatize)\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1000, lowercase=True, stop_words=stop_words_spanish)\n",
    "X = vectorizer.fit_transform(df2['cleaned_content'])\n",
    "\n",
    "\n",
    "num_topics = 8  # select number of topics\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "\n",
    "df2['topic_probabilities'] = lda.transform(X).tolist()\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "prueba = pyLDAvis.lda_model.prepare(lda, X, vectorizer, mds='tsne', n_jobs=1)\n",
    "pyLDAvis.save_html(prueba, \"2012-2016_tsne.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeled according to media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Litoral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_el = df[df[\"media\"] == \"litoral\"]\n",
    "\n",
    "# defining stopwords\n",
    "stop_words_spanish = spacy.lang.es.stop_words.STOP_WORDS\n",
    "\n",
    "# lemmatization\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc if token.text.lower() not in stop_words_spanish])\n",
    "\n",
    "df_el['cleaned_content'] = df_el['cleaned_content'].apply(lemmatize)\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1000, lowercase=True, stop_words=stop_words_spanish)\n",
    "X = vectorizer.fit_transform(df_el['cleaned_content'])\n",
    "\n",
    "\n",
    "num_topics = 8  # select number of topics\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "\n",
    "df_el['topic_probabilities'] = lda.transform(X).tolist()\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "prueba = pyLDAvis.lda_model.prepare(lda, X, vectorizer, mds='tsne', n_jobs=1)\n",
    "pyLDAvis.save_html(prueba, \"ellitoral.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Página 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p12 = df[df[\"media\"] == \"p12\"]\n",
    "\n",
    "\n",
    "stop_words_spanish = spacy.lang.es.stop_words.STOP_WORDS\n",
    "\n",
    "\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc if token.text.lower() not in stop_words_spanish])\n",
    "\n",
    "\n",
    "df_p12['cleaned_content'] = df_p12['cleaned_content'].apply(lemmatize)\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1000, lowercase=True, stop_words=stop_words_spanish)\n",
    "X = vectorizer.fit_transform(df_p12['cleaned_content'])\n",
    "\n",
    "\n",
    "num_topics = 8  # select number of topics\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "\n",
    "df_p12['topic_probabilities'] = lda.transform(X).tolist()\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "prueba = pyLDAvis.lda_model.prepare(lda, X, vectorizer, mds='tsne', n_jobs=1)\n",
    "pyLDAvis.save_html(prueba, \"p12.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
